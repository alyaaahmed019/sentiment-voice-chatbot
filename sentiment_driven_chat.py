# -*- coding: utf-8 -*-
"""Sentiment-Driven_Chat.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18hVDL5L7hhou815scCgTLOciz8aTVtN2
"""

from datasets import load_dataset
ds = load_dataset("cardiffnlp/tweet_eval", "emotion")

import pandas as pd
df = ds['train'].to_pandas()
df

df['label'].value_counts()

import re
import string
def clean_text(text):
    text = text.lower()
    text = re.sub('\[".*\]', '', text)   # remove text in square brackets
    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)  # remove punctuation
    text = re.sub('\w*\d\w*', '', text)   # remove words containing numbers
    text = re.sub(r'https?://\S+|www\.\S+', '', text)  # remove URLs
    text = re.sub(r'\s+', ' ', text).strip()
    return text
df['text']= df['text'].apply(clean_text)

df['text']

df['label'].info()

def convert_labels(y):
    label_map = {0: 'anger', 1: 'joy', 2: 'optimism', 3: 'sadness'}
    return label_map.get(y, 'unknown')

df['label'] = df['label'].apply(convert_labels)

df

!pip install faiss-cpu

#Embedding
from sentence_transformers import SentenceTransformer
import numpy as np
import faiss

model = SentenceTransformer('all-MiniLM-L6-v2')
df_embeddings = model.encode(df['text'].tolist())
print(f"Document embeddings shape: {df.shape}")

index = faiss.IndexFlatL2(df_embeddings.shape[1])
index.add(df_embeddings)

def retrieve_data(query, k=3, return_distances=True):
    query_embedding = model.encode([query])
    distances, indices = index.search(query_embedding, k)

    if return_distances:
        return [(df['text'].iloc[i], float(distances[0][j])) for j, i in enumerate(indices[0])]
    else:
        return [df['text'].iloc[i] for i in indices[0]]

df['data_retrieved'] = df['text'].apply(lambda x: retrieve_data(x, k=3))

print(df['data_retrieved'].iloc[0])

df

from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModelForSeq2SeqLM, T5Tokenizer
import torch

gen_name = "google/flan-t5-large"   # or flan-t5-base for smaller
gen_tokenizer = T5Tokenizer.from_pretrained(gen_name)
generator = AutoModelForSeq2SeqLM.from_pretrained(gen_name)
#generator.eval()

from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModelForSeq2SeqLM, T5Tokenizer
import torch


def detect_emotion(text, retrieved_data=None, model_name='google/flan-t5-large'):
    # Use the pre-loaded tokenizer and generator
    global gen_tokenizer, generator
    few_shot_prompt = """
You are an expert at emotion classification.
You can classify text into one of: anger, joy, optimism, or sadness.

Examples:
Text: "I canâ€™t believe they ignored me again, this is so frustrating!"
Emotion: anger

Text: "I just got the job, Iâ€™m so happy right now!"
Emotion: joy

Text: "Things are tough now, but I know theyâ€™ll get better soon."
Emotion: optimism

Text: "I feel so alone, nobody seems to understand me."
Emotion: sadness

Now, classify the new text.
"""

    # Base prompt
    prompt = few_shot_prompt + f"\nText: {text}\nEmotion:"
    # Incorporate retrieved data into the prompt if available
    if retrieved_data:
        # Assuming retrieved_data is a list of strings (the retrieved text)
        context = " ".join(retrieved_data)
        prompt = few_shot_prompt + f"\nContext: {context}\nText: {text}\nEmotion:"
    inputs = gen_tokenizer.encode(prompt, return_tensors="pt", max_length=512, truncation=True) # Increased max_length

    with torch.no_grad():
        outputs = generator.generate(
            inputs,
            max_length=100,
            num_return_sequences=1,
            temperature=0.1,
            do_sample=True
        )

    response = gen_tokenizer.decode(outputs[0], skip_special_tokens=True).lower().strip()
    emotions = ['anger', 'joy', 'optimism', 'sadness']

    for emotion in emotions:
        if emotion in response:
            return emotion
    return 'neutral'

# Test the RAG pipeline with a query
query = "If we keep trying, success is just around the corner"

# Retrieve relevant document embeddings
retrieved_data_tuples = retrieve_data(query, k=3)

# Extract the text from the retrieved data tuples
retrieved_docs_text = [text for text, distance in retrieved_data_tuples]

# Pass the query and the retrieved text to the emotion detection function
emotion= detect_emotion(query, retrieved_docs_text)
print(f"Query: {query}")
print(f"Detected Emotion: {emotion}")

!pip install streamlit

!pip install pyngrok

!pkill -f ngrok

"""##Speech to text"""

!pip install transformers datasets torch scipy librosa
!apt-get update && apt-get install -y ffmpeg
!pip install gtts

from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor
import torch
import librosa
from IPython.display import Javascript, display, Audio
from google.colab import output
from base64 import b64decode
import numpy as np
import time

# JavaScript code to record audio in the browser
RECORD = """
const sleep = time => new Promise(resolve => setTimeout(resolve, time));
const b2text = blob => new Promise(resolve => {
  const reader = new FileReader();
  reader.onloadend = e => resolve(e.srcElement.result);
  reader.readAsDataURL(blob);
});
var record = time => new Promise(async resolve => {
  stream = await navigator.mediaDevices.getUserMedia({ audio: true });
  recorder = new MediaRecorder(stream);
  chunks = [];
  recorder.ondataavailable = e => chunks.push(e.data);
  recorder.start();
  await sleep(time);
  recorder.onstop = async () => {
    blob = new Blob(chunks, { type: 'audio/wav' });
    text = await b2text(blob);
    resolve(text);
  };
  recorder.stop();
});
"""

def record_audio(duration=10):
    print("Preparing to record... Please wait.")
    time.sleep(2)  # Short delay to ensure user is ready
    print("Speak now!")
    display(Javascript(RECORD))
    print(f"Recording for {duration} seconds...")
    s = output.eval_js(f'record({duration * 1000})')
    b = b64decode(s.split(',')[1])
    audio_file = 'output.wav'
    with open(audio_file, 'wb') as f:
        f.write(b)
    print("Recording finished and saved as", audio_file)
    return audio_file

# Load pre-trained Wav2Vec model and processor
processor = Wav2Vec2Processor.from_pretrained("facebook/wav2vec2-base-960h")
model_voice= Wav2Vec2ForCTC.from_pretrained("facebook/wav2vec2-base-960h")
# Record audio
try:
    audio_file = record_audio(duration=10)
except Exception as e:
    print(f"Error recording audio: {e}")
    print("Please ensure microphone access is enabled in your browser.")
    raise

# Validate audio file
try:
    audio_input, sr = librosa.load(audio_file, sr=16000)
    print(f"Audio loaded successfully. Duration: {librosa.get_duration(y=audio_input, sr=sr)} seconds")
    # Check if audio is silent
    if np.max(np.abs(audio_input)) < 0.01:  # Threshold for silence
        print("Warning: Audio appears to be silent or very quiet. Try speaking louder or closer to the microphone.")
except Exception as e:
    print(f"Error loading audio file: {e}")
    raise

# Preprocess audio
input_values = processor(audio_input, return_tensors="pt", sampling_rate=16000).input_values

# Perform inference
with torch.no_grad():
    logits = model_voice(input_values).logits

# Decode the transcription
predicted_ids = torch.argmax(logits, dim=-1)
transcription = processor.batch_decode(predicted_ids)[0]
print("Transcription:", transcription)

#Embedding
from sentence_transformers import SentenceTransformer
import numpy as np
import faiss

model = SentenceTransformer('all-MiniLM-L6-v2')
df_embeddings = model.encode(df['text'].tolist())
print(f"Document embeddings shape: {df.shape}")

index = faiss.IndexFlatL2(df_embeddings.shape[1])
index.add(df_embeddings)

def retrieve_data(query, k=3, return_distances=True):
    query_embedding = model.encode([query])
    distances, indices = index.search(query_embedding, k)

    if return_distances:
        return [(df['text'].iloc[i], float(distances[0][j])) for j, i in enumerate(indices[0])]
    else:
        return [df['text'].iloc[i] for i in indices[0]]

from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModelForSeq2SeqLM, T5Tokenizer
import torch

def detect_emotion(text, retrieved_data=None, model_name='google/flan-t5-large'):
    # Use the pre-loaded tokenizer and generator
    global gen_tokenizer, generator
    few_shot_prompt = """
You are an expert at emotion classification.
You can classify text into one of: anger, joy, optimism, or sadness.

Examples:
Text: "I canâ€™t believe they ignored me again, this is so frustrating!"
Emotion: anger

Text: "I just got the job, Iâ€™m so happy right now!"
Emotion: joy

Text: "Things are tough now, but I know theyâ€™ll get better soon."
Emotion: optimism

Text: "I feel so alone, nobody seems to understand me."
Emotion: sadness

Now, classify the new text.
"""

    # Base prompt
    prompt = few_shot_prompt + f"\nText: {text}\nEmotion:"
    # Incorporate retrieved data into the prompt if available
    if retrieved_data:
        # Assuming retrieved_data is a list of strings (the retrieved text)
        context = " ".join(retrieved_data)
        prompt = few_shot_prompt + f"\nContext: {context}\nText: {text}\nEmotion:"
    inputs = gen_tokenizer.encode(prompt, return_tensors="pt", max_length=512, truncation=True) # Increased max_length

    with torch.no_grad():
        outputs = generator.generate(
            inputs,
            max_length=100,
            num_return_sequences=1,
            temperature=0.1,
            do_sample=True
        )

    response = gen_tokenizer.decode(outputs[0], skip_special_tokens=True).lower().strip()
    emotions = ['anger', 'joy', 'optimism', 'sadness']

    for emotion in emotions:
        if emotion in response:
            return emotion
    return 'neutral'

# Test the RAG pipeline with a query

# Retrieve relevant document embeddings
retrieved_data_tuples = retrieve_data(transcription, k=3)

# Extract the text from the retrieved data tuples
retrieved_docs_text = [text for text, distance in retrieved_data_tuples]

# Pass the query and the retrieved text to the emotion detection function
emotion= detect_emotion(transcription, retrieved_docs_text)
print(f"Query: {transcription}")
print(f"Detected Emotion: {emotion}")

from gtts import gTTS
from IPython.display import Audio, display
import os

def speak_text(text, lang="en", filename="output_tts.mp3", autoplay=True):
    if not text:
        text = "Hello, this is a test speech."

    # Generate TTS
    tts = gTTS(text=text, lang=lang)
    tts.save(filename)
    # Play the audio
    audio = Audio(filename, autoplay=autoplay)
    display(audio)
    return audio

audio = speak_text(f"detected feeling is {emotion}")

# Commented out IPython magic to ensure Python compatibility.
# %%writefile sentiment_voice_app.py
# import streamlit as st
# import torch
# import faiss
# import numpy as np
# from gtts import gTTS
# from transformers import T5Tokenizer, AutoModelForSeq2SeqLM
# from sentence_transformers import SentenceTransformer
# from io import BytesIO
# 
# # ---------------------------
# # Load models once
# # ---------------------------
# @st.cache_resource
# def load_models():
#     embedder = SentenceTransformer("all-MiniLM-L6-v2")
#     tokenizer = T5Tokenizer.from_pretrained("google/flan-t5-large")
#     generator = AutoModelForSeq2SeqLM.from_pretrained("google/flan-t5-large")
#     return embedder, tokenizer, generator
# 
# model, gen_tokenizer, generator = load_models()
# 
# # ---------------------------
# # FAISS index setup (dummy example, replace with your df)
# # ---------------------------
# # Example dataset
# example_texts = [
#     "I am really angry about this situation!",
#     "This is the best day ever, Iâ€™m so happy!",
#     "I believe things will work out fine.",
#     "I feel very lonely and sad today."
# ]
# 
# df = {"text": example_texts}
# df_embeddings = model.encode(example_texts)
# index = faiss.IndexFlatL2(df_embeddings.shape[1])
# index.add(df_embeddings)
# 
# # ---------------------------
# # Retrieval function
# # ---------------------------
# def retrieve_data(query, k=3, return_distances=True):
#     query_embedding = model.encode([query])
#     distances, indices = index.search(query_embedding, k)
#     if return_distances:
#         return [(df["text"][i], float(distances[0][j])) for j, i in enumerate(indices[0])]
#     else:
#         return [df["text"][i] for i in indices[0]]
# 
# # ---------------------------
# # Emotion detection function
# # ---------------------------
# def detect_emotion(text, retrieved_data=None):
#     few_shot_prompt = """
# You are an expert at emotion classification.
# You can classify text into one of: anger, joy, optimism, or sadness.
# 
# Examples:
# Text: "I canâ€™t believe they ignored me again, this is so frustrating!"
# Emotion: anger
# 
# Text: "I just got the job, Iâ€™m so happy right now!"
# Emotion: joy
# 
# Text: "Things are tough now, but I know theyâ€™ll get better soon."
# Emotion: optimism
# 
# Text: "I feel so alone, nobody seems to understand me."
# Emotion: sadness
# 
# Now, classify the new text.
# """
#     prompt = few_shot_prompt + f"\nText: {text}\nEmotion:"
#     if retrieved_data:
#         context = " ".join([d[0] for d in retrieved_data])
#         prompt = few_shot_prompt + f"\nContext: {context}\nText: {text}\nEmotion:"
# 
#     inputs = gen_tokenizer.encode(prompt, return_tensors="pt", max_length=512, truncation=True)
# 
#     with torch.no_grad():
#         outputs = generator.generate(
#             inputs,
#             max_length=100,
#             num_return_sequences=1,
#             temperature=0.1,
#             do_sample=True
#         )
# 
#     response = gen_tokenizer.decode(outputs[0], skip_special_tokens=True).lower().strip()
#     emotions = ["anger", "joy", "optimism", "sadness"]
# 
#     for emotion in emotions:
#         if emotion in response:
#             return emotion
#     return "neutral"
# 
# # ---------------------------
# # Text-to-Speech function
# # ---------------------------
# def speak_text(text, lang="en"):
#     if not text:
#         text = "Hello, this is a test speech."
#     tts = gTTS(text=text, lang=lang)
#     mp3_fp = BytesIO()
#     tts.write_to_fp(mp3_fp)
#     return mp3_fp
# 
# st.title("ðŸŽ¤ EmpathyBot: Sentiment-Driven Voice Chat")
# 
# # Input: text or voice
# st.subheader("ðŸ“ Enter your message")
# user_input = st.text_area("Type a message here:")
# 
# st.subheader("ðŸŽ™ï¸ Or upload a voice message")
# uploaded_file = st.file_uploader("Upload a WAV/MP3 file", type=["wav", "mp3"])
# 
# if st.button("Analyze"):
#     if user_input.strip() == "" and uploaded_file is None:
#         st.warning("Please provide either text or a voice file.")
#     else:
#         # If voice uploaded, just placeholder (STT can be added with Whisper)
#         if uploaded_file:
#             st.audio(uploaded_file, format="audio/wav")
#             user_input = "This is a placeholder transcription from audio."  # Replace with real STT
# 
#         retrieved = retrieve_data(user_input, k=3)
#         emotion = detect_emotion(user_input, retrieved)
# 
#         st.subheader("ðŸ”Ž Detected Emotion")
#         st.success(f"**{emotion.upper()}**")
# 
#         st.subheader("ðŸ“š Retrieved Responses")
#         for i, (doc, dist) in enumerate(retrieved, 1):
#             st.write(f"{i}. {doc} (distance={dist:.4f})")
# 
#         # Generate speech
#         st.subheader("ðŸ”Š Voice Response")
#         mp3_fp = speak_text(f"The detected emotion is {emotion}")
#         st.audio(mp3_fp, format="audio/mp3")
#

# from pyngrok import ngrok, conf
# # Replace with your token
# NGROK_AUTH_TOKEN = "32WErbXm92Uc0ODRnLo6eftQWUA_MNhgheS92ArigVnpNZno"

# # Expose the Streamlit app through ngrok
# conf.get_default().auth_token = NGROK_AUTH_TOKEN # Set the authtoken
# public_url = ngrok.connect('8080')
# print(f"Streamlit app is live at: {public_url}")

!streamlit run sentiment_voice_app.py & npx localtunnel --port 8501

